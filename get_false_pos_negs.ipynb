{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36ff105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "import pickle\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "n_splits = 10\n",
    "results_dict = {}  # Store mean/std r² per i and n_timesteps\n",
    "best_models = {}   # Store best model per (i, n_timesteps)\n",
    "T = 5\n",
    "fig, ax = plt.subplots()\n",
    "for i, (resampled_onset, exp_onset) in enumerate(zip(resampled_onsets, exp_onsets)):\n",
    "    score_means = []\n",
    "    score_stds = []\n",
    "    # all_r2_means = []\n",
    "    all_scores_per_i = {}\n",
    "    n_timesteps_list = []\n",
    "    \n",
    "\n",
    "    # for n_timesteps in range(5, 10, 2):\n",
    "    for n_timesteps in range(1, 15, 1):\n",
    "        print(f\"\\nProcessing i={i}, n_timesteps={n_timesteps}\")\n",
    "        feature_names = np.array([[f\"vel_t-{n_timesteps - t - 1}\", f\"accel_t-{n_timesteps - t - 1}\",\n",
    "                                   f\"curv_t-{n_timesteps - t - 1}\", f\"beh_t-{n_timesteps - t - 1}\"]\n",
    "                                  for t in range(n_timesteps)]).flatten()\n",
    "\n",
    "        X_stim = prep_FB_inputs(resampled_vel, resampled_acc, resampled_curve, resampled_rev,\n",
    "                                resampled_turn, T=n_timesteps)  # shape (n_tracks, n_frames, n_features)\n",
    "        \n",
    "        rev_bin = exp_behaviors\n",
    "        X_stim_all = X_stim[:, resampled_onset - T, :]\n",
    "\n",
    "        \n",
    "        Y_latency = latency_to_reversal(rev_bin, exp_onset, max_latency = 6*durations[i]+1)/6 # nan where not \n",
    "        no_rev_at_onset = Y_latency!=0\n",
    "        Y_prob = np.logical_not(np.isnan(Y_latency[no_rev_at_onset]))# classfy y/n did rev happen\n",
    "        X_prob = copy.deepcopy(X_stim_all[no_rev_at_onset])\n",
    "        \n",
    "        ###    \n",
    "        # X_train, X_test, y_train, y_test = train_test_split(X_prob, Y_prob, test_size=test_size, random_state=random_state)\n",
    "        balanced_accuracy_scores = []\n",
    "        models = []\n",
    "\n",
    "        for split_idx in range(n_splits):\n",
    "            # X_train, X_test, y_train, y_test = train_test_split(\n",
    "            #     X_stim_all_latency, Y_latency_valid, test_size=test_size, random_state=split_idx)\n",
    "            \n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_prob, Y_prob, test_size=test_size, random_state=split_idx)\n",
    "            \n",
    "            param_grid = {\n",
    "                'C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "            }\n",
    "\n",
    "            model = LinearSVC(penalty='l1', dual=False, max_iter=10000)\n",
    "            # Grid search\n",
    "            grid = GridSearchCV(model, param_grid, cv=5, scoring='balanced_accuracy')\n",
    "            grid.fit(X_train, y_train)\n",
    "            c = grid.best_params_['C']\n",
    "            best_model = grid.best_estimator_\n",
    "            \n",
    "            y_pred = best_model.predict(X_test)\n",
    "            score =balanced_accuracy_score(y_test, y_pred)  \n",
    "\n",
    "            balanced_accuracy_scores.append(score)\n",
    "            # models.append(best_model)\n",
    "            models.append((best_model,X_train, X_test, y_train, y_test, c))\n",
    "\n",
    "        accuracy_mean = np.mean(balanced_accuracy_scores)\n",
    "        accuracy_std = np.std(balanced_accuracy_scores)\n",
    "        best_model_idx = np.argmax(balanced_accuracy_scores)\n",
    "        best_model_overall = models[best_model_idx]\n",
    "\n",
    "        # Save results\n",
    "        n_timesteps_list.append(n_timesteps)\n",
    "        # all_r2_means.append(r2s)\n",
    "        all_scores_per_i[(i, n_timesteps)] = balanced_accuracy_scores\n",
    "        score_means.append(accuracy_mean)\n",
    "        score_stds.append(accuracy_std)\n",
    "        results_dict[(i, n_timesteps)] = (accuracy_mean, accuracy_std)\n",
    "        best_models[(i, n_timesteps)] = best_model_overall\n",
    "\n",
    "\n",
    "    # cmap1 = cm.get_cmap('tab10', len(resampled_onsets))\n",
    "    ax.errorbar(n_timesteps_list, score_means, yerr=score_stds, #color=cmap1(i),\n",
    "                fmt='-o', capsize=5, label = f\"stim {i}\")\n",
    "\n",
    "    ax.set_title(f\"{neuron}; Reversal prediction; Mean ± STD balanced accuracy\")\n",
    "    ax.set_xlabel(\"max time delay (frames)\")\n",
    "    ax.set_ylabel(\"balanced accuracy\")\n",
    "ax.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f\"r2_plot_i{i}.png\")\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1bb845",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###get inputs into lstm \n",
    "beh_map = [z_norm.min(), z_norm.min()/2 + z_norm.max()/2, z_norm.max()]\n",
    "\n",
    "def resample_fps(feature_arr, target_fps,  original_fps):\n",
    "    # Resample using linear interpolation\n",
    "    indices = np.arange(len(feature_arr))\n",
    "    new_indices = np.linspace(0, len(feature_arr) - 1, int(len(feature_arr) * (target_fps / original_fps)))\n",
    "    resampled_values = np.interp(new_indices, indices, feature_arr)\n",
    "    return resampled_values\n",
    "\n",
    "def resample_2d(inferred_phases_all_shifted, target_fps,  original_fps):\n",
    "    n_tracks = inferred_phases_all_shifted.shape[0]\n",
    "    resampled_tracks = []\n",
    "    for track_i in range(n_tracks):\n",
    "        resampled = resample_fps(inferred_phases_all_shifted[track_i, :], target_fps,  original_fps)\n",
    "        resampled_tracks.append(resampled[None,:])\n",
    "\n",
    "    inferred_phases_all_shifted_high_fps = np.concatenate(resampled_tracks, axis= 0 )\n",
    "    return inferred_phases_all_shifted_high_fps\n",
    "\n",
    "\n",
    "def feature_all_to_resampled(inferred_phases_all, n_tracks):\n",
    "   \n",
    "    inferred_phases_all = inferred_phases_all.reshape(n_tracks, -1)\n",
    "   \n",
    "    n_timesteps = inferred_phases_all.shape[1]\n",
    "    print(\"n_timesteps\", n_timesteps)\n",
    "    # inferred_phases_all_shifted = np.zeros((n_tracks,n_timesteps+5+8))+np.nan #ask Bennet about the 8 extra time steps missing\n",
    "    # inferred_phases_all_shifted[:, 5:-8] = inferred_phases_all\n",
    "    \n",
    "    inferred_phases_all_shifted = np.zeros((n_tracks,n_timesteps+5+8+8))+np.nan #ask Bennet about the 8 extra time steps missing\n",
    "    inferred_phases_all_shifted[:, 5:-16] = inferred_phases_all\n",
    "    return inferred_phases_all_shifted\n",
    "\n",
    "def flatten_and_remove_nans(resampled_features):\n",
    "    \n",
    "    # return resampled_features[:, 5:-8].flatten()\n",
    "    return resampled_features.flatten()\n",
    "\n",
    "\n",
    "# n_tracks  = resampled_vel.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "# print(phase_resampled.shape)\n",
    "# print(radii_resampled.shape)\n",
    "\n",
    "\n",
    "def prep_FB_inputs_donut_only( inferred_phases_all, inferred_rad_all, resampled_onset, n_tracks):\n",
    "    phase_resampled = feature_all_to_resampled(inferred_phases_all, n_tracks)\n",
    "    radii_resampled = feature_all_to_resampled(inferred_rad_all, n_tracks) \n",
    "    \n",
    "    X_donut = np.concatenate(phase_resampled[:, :, None] ,  radii_resampled[:, :, None], axis = 2)\n",
    "    X_donut_stim = X_donut[:, resampled_onset,:]\n",
    "    \n",
    "    feature_names = [\"phase\", \"radius\"]\n",
    "    return X_donut_stim, feature_names\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def prep_FB_inputs_features_only(resampled_vel, resampled_acc, resampled_curve, resampled_rev, resampled_turn, resampled_onset,   T = 5\n",
    "                #    times, beh_map, \n",
    "                #    lag=16, inclusion_thresh =3, \n",
    "                #    remove_revs=False, \n",
    "                  \n",
    "                   ): \n",
    "\n",
    "    behavior_input = np.array(resampled_rev+2*resampled_turn, dtype=np.float64)\n",
    "    behavior_input -= np.array(z).mean()\n",
    "    behavior_input /= np.array(z).std()\n",
    "\n",
    "    X_all_LSTM = []\n",
    "    for new_worm_idx in range(len(resampled_vel)):\n",
    "       \n",
    "        X_new_worm = np.stack([resampled_vel[new_worm_idx], resampled_acc[new_worm_idx], resampled_curve[new_worm_idx], behavior_input[new_worm_idx]], axis=1)  \n",
    "        # X_new_worm = np.stack([resampled_vel[new_worm_idx], resampled_acc[new_worm_idx], behavior_input[new_worm_idx]], axis=1)  \n",
    "        X_new_tensor = torch.tensor(X_new_worm, dtype=torch.float32)\n",
    "        \n",
    "        if T > 0:\n",
    "            X_new_seq1 = create_X_sequences(X_new_tensor, T).numpy() #torch.Size([475, 5, 4]), = n_frames, time delay, 4 is the feature  #\n",
    "        else: \n",
    "            X_new_seq1 = X_new_tensor.numpy()\n",
    "        \n",
    "        n_frames, delay, n_features = X_new_seq1.shape\n",
    "        X_new_seq1 = X_new_seq1.reshape((n_frames,  delay*n_features)) # check how this is shaped it will be f1_t-T, f2_t-T, f3_t-T, f4_t-T, ; f1_t-T+1, f2_T+1, f3_T+1 f4_T+1\n",
    "        \n",
    "            \n",
    "        X_all_LSTM.append(X_new_seq1)\n",
    "    # X_all_LSTM = np.concatenate(X_all_LSTM, axis = 1 ) # i guess should check if reshape how it reshapes.. ie if feautres are all together or not \n",
    "    X_all_LSTM = np.array(X_all_LSTM)#shape (n_tracks, n_frames, n_features)\n",
    "    feature_names = np.array([[f\"vel_t-{T - t - 1}\", f\"accel_t-{T - t - 1}\",\n",
    "                                   f\"curv_t-{T - t - 1}\", f\"beh_t-{T - t - 1}\"]\n",
    "                                  for t in range(T)]).flatten()\n",
    "    \n",
    "    X_stim_features = X_all_LSTM[:, resampled_onset - T, :]\n",
    "\n",
    "    return X_stim_features, feature_names\n",
    "\n",
    "\n",
    "    \n",
    "def prep_FB_inputs_feature_and_donut(resampled_vel, resampled_acc, resampled_curve, resampled_rev, resampled_turn, inferred_phases_all, inferred_rad_all,resampled_onset, n_tracks,   T = 5):\n",
    "    \n",
    "    X_stim_features, feature_names = prep_FB_inputs_features_only(resampled_vel, resampled_acc, resampled_curve, resampled_rev, resampled_turn, resampled_onset,  T = T)#shape (n_tracks, n_frames, n_features)\n",
    "    X_donut_stim, donut_names = prep_FB_inputs_donut_only( inferred_phases_all, inferred_rad_all, resampled_onset, n_tracks)\n",
    "\n",
    "    X_stim_all =  np.concatenate(X_stim_features[:, :, None] ,  X_donut_stim[:, :, None], axis = 2)\n",
    "    feature_names_all = feature_names+donut_names\n",
    "    return X_stim_all, feature_names_all\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i, (resampled_onset, exp_onset) in enumerate(zip(resampled_onsets, exp_onsets)):\n",
    "    \n",
    "    np.concatenate(inferred_phases[idxs[i]]), np.concatenate(inferred_rad[idxs[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d571d346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_FB_inputs_features_only(resampled_vel, resampled_acc, resampled_curve, resampled_rev, resampled_turn, resampled_onset,   T = 5\n",
    "                #    times, beh_map, \n",
    "                #    lag=16, inclusion_thresh =3, \n",
    "                #    remove_revs=False, \n",
    "                  \n",
    "                   ): \n",
    "\n",
    "    behavior_input = np.array(resampled_rev+2*resampled_turn, dtype=np.float64)\n",
    "    behavior_input -= np.array(z).mean()\n",
    "    behavior_input /= np.array(z).std()\n",
    "\n",
    "    X_all_LSTM = []\n",
    "    for new_worm_idx in range(len(resampled_vel)):\n",
    "       \n",
    "        X_new_worm = np.stack([resampled_vel[new_worm_idx], resampled_acc[new_worm_idx], resampled_curve[new_worm_idx], behavior_input[new_worm_idx]], axis=1)  \n",
    "        # X_new_worm = np.stack([resampled_vel[new_worm_idx], resampled_acc[new_worm_idx], behavior_input[new_worm_idx]], axis=1)  \n",
    "        X_new_tensor = torch.tensor(X_new_worm, dtype=torch.float32)\n",
    "        \n",
    "        if T > 0:\n",
    "            X_new_seq1 = create_X_sequences(X_new_tensor, T).numpy() #torch.Size([475, 5, 4]), = n_frames, time delay, 4 is the feature  #\n",
    "        else: \n",
    "            X_new_seq1 = X_new_tensor.numpy()\n",
    "        \n",
    "        n_frames, delay, n_features = X_new_seq1.shape\n",
    "        X_new_seq1 = X_new_seq1.reshape((n_frames,  delay*n_features)) # check how this is shaped it will be f1_t-T, f2_t-T, f3_t-T, f4_t-T, ; f1_t-T+1, f2_T+1, f3_T+1 f4_T+1\n",
    "        \n",
    "            \n",
    "        X_all_LSTM.append(X_new_seq1)\n",
    "    # X_all_LSTM = np.concatenate(X_all_LSTM, axis = 1 ) # i guess should check if reshape how it reshapes.. ie if feautres are all together or not \n",
    "    X_all_LSTM = np.array(X_all_LSTM)#shape (n_tracks, n_frames, n_features)\n",
    "    feature_names = np.array([[f\"vel_t-{T - t - 1}\", f\"accel_t-{T - t - 1}\",\n",
    "                                   f\"curv_t-{T - t - 1}\", f\"beh_t-{T - t - 1}\"]\n",
    "                                  for t in range(T)]).flatten()\n",
    "    \n",
    "    X_stim_features = X_all_LSTM[:, resampled_onset - T, :]\n",
    "\n",
    "    return X_stim_features, feature_names\n",
    "\n",
    "\n",
    "\n",
    "def prep_FB_inputs(resampled_vel, resampled_acc, resampled_curve, resampled_rev, resampled_turn,  T = 5\n",
    "                #    times, beh_map, \n",
    "                #    lag=16, inclusion_thresh =3, \n",
    "                #    remove_revs=False, \n",
    "                  \n",
    "                   ): \n",
    "    #num time lags\n",
    "\n",
    "    # if remove_revs:\n",
    "    #     rev_id = beh_map[1]\n",
    "    # else:\n",
    "    #     rev_id = 100000\n",
    "\n",
    "    behavior_input = np.array(resampled_rev+2*resampled_turn, dtype=np.float64)\n",
    "    behavior_input -= np.array(z).mean()\n",
    "    behavior_input /= np.array(z).std()\n",
    "\n",
    "    # inferred_phases = np.zeros((len(times), len(resampled_vel))) +np.nan\n",
    "    # inferred_rad = np.zeros((len(times), len(resampled_vel))) +np.nan\n",
    "    # final_behaviors = np.zeros((len(times), len(resampled_vel))) +np.nan\n",
    "    # initial_behaviors = np.zeros((len(times), len(resampled_vel))) +np.nan\n",
    "    # initial_cts_beh = np.zeros((len(times), len(resampled_vel))) +np.nan\n",
    "\n",
    "    # final_behaviors_all = []\n",
    "    # inferred_phases_all = []\n",
    "    # inferred_rad_all = []\n",
    "    # behaviors_all = []\n",
    "    X_all_LSTM = []\n",
    "    for new_worm_idx in range(len(resampled_vel)):\n",
    "       \n",
    "        X_new_worm = np.stack([resampled_vel[new_worm_idx], resampled_acc[new_worm_idx], resampled_curve[new_worm_idx], behavior_input[new_worm_idx]], axis=1)  \n",
    "        # X_new_worm = np.stack([resampled_vel[new_worm_idx], resampled_acc[new_worm_idx], behavior_input[new_worm_idx]], axis=1)  \n",
    "        X_new_tensor = torch.tensor(X_new_worm, dtype=torch.float32)\n",
    "        \n",
    "        if T > 0:\n",
    "            X_new_seq1 = create_X_sequences(X_new_tensor, T).numpy() #torch.Size([475, 5, 4]), = n_frames, time delay, 4 is the feature  #\n",
    "        else: \n",
    "            X_new_seq1 = X_new_tensor.numpy()\n",
    "        \n",
    "        n_frames, delay, n_features = X_new_seq1.shape\n",
    "        X_new_seq1 = X_new_seq1.reshape((n_frames,  delay*n_features)) # check how this is shaped it will be f1_t-T, f2_t-T, f3_t-T, f4_t-T, ; f1_t-T+1, f2_T+1, f3_T+1 f4_T+1\n",
    "        \n",
    "        # # Normalize the inputs\n",
    "        # X_train_mean, X_train_std = X_new_seq1.mean(axis=0), X_new_seq1.std(axis=0)\n",
    "        # if normalize:\n",
    "        #     X_new_seq1 = (X_new_seq1 - X_train_mean) / (X_train_std + 1e-8)  \n",
    "            \n",
    "        X_all_LSTM.append(X_new_seq1)\n",
    "    # X_all_LSTM = np.concatenate(X_all_LSTM, axis = 1 ) # i guess should check if reshape how it reshapes.. ie if feautres are all together or not \n",
    "    X_all_LSTM = np.array(X_all_LSTM)#shape (n_tracks, n_frames, n_features)\n",
    "    return X_all_LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c96a9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_stim_all, feature_names_all = prep_FB_inputs_features_only(resampled_vel, resampled_acc, resampled_curve, resampled_rev, resampled_turn, onset, n_timesteps,   lstm_lag = lstm_lag)#shape (n_tracks, n_frames, n_features)\n",
    "X_stim_all, feature_names_all =  prep_FB_inputs_feature_and_donut(resampled_vel, resampled_acc, resampled_curve, resampled_rev, resampled_turn, inferred_phases_all, inferred_rad_all,resampled_onset, n_tracks,   n_timesteps, lstm_lag = 5)\n",
    "n_timesteps = 11\n",
    "# feature_names = np.array([[f\"vel_t-{n_timesteps - t - 1}\", f\"accel_t-{n_timesteps - t - 1}\",\n",
    "#                             f\"curv_t-{n_timesteps - t - 1}\", f\"beh_t-{n_timesteps - t - 1}\"]\n",
    "#                             for t in range(n_timesteps)]).flatten()\n",
    "for i in range(len(resampled_onsets)):\n",
    "    model,X_train, X_test, y_train, y_test, c  = best_models[(i, n_timesteps)] \n",
    "    \n",
    "    \n",
    "    X_stim_features, feature_names = prep_FB_inputs_features_only(resampled_vel, resampled_acc, resampled_curve, resampled_rev, resampled_turn, resampled_onset,   n_timesteps, lstm_lag = lstm_lag)#shape (n_tracks, n_frames, n_features)\n",
    "    X_donut_stim, donut_names = prep_FB_inputs_donut_only( inferred_phases_all, inferred_rad_all, resampled_onset, n_tracks)\n",
    "    X_stim_all =  np.concatenate([X_stim_features ,  X_donut_stim], axis = 1)\n",
    "    feature_names_all = np.array(feature_names.tolist()+donut_names)\n",
    "\n",
    "    model_label= f\"{neuron} stim: {i}; linear SVC; donut and features\"#; lasso_a{np.round(alpha,2)}\"\n",
    "    visualize_model_classification(model, model_label, feature_names_all, X_train, X_test, y_train, y_test, n_timesteps,  feature_names_ordered = False, coeffs = model.coef_[0])#, xlim = [-1,1])\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6d7bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "pdf_save_dir = \"/Users/friederikebuck/Downloads/worm notes/rslds_all_dates/\"\n",
    "os.makedirs(pdf_save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "exp_dates = get_exp_dates() \n",
    "colors = [\"dodgerblue\", \"magenta\", \"navy\", \"orange\", \"green\"]\n",
    "all_figs = []\n",
    "for exp_i, (date, z_w, q_z_w, labels, traces) in enumerate(zip(exp_dates, z, q_z, full_neural_labels, full_traces)):#(len(z)):\n",
    "    # Plot the true and inferred states\n",
    "    \n",
    "    n_sections = 4\n",
    "    # fig, axs = plt.subplots(2,1, figsize=(18,6))\n",
    "    fig, axs = plt.subplots(2*n_sections,1, figsize=(18,6*n_sections))\n",
    "    axs[0].imshow(z_w[None,:], aspect=\"auto\", cmap=cmap, alpha=0.3, vmin=0, vmax=len(palette))\n",
    "    axs[1].imshow(q_z_w[None,:], aspect=\"auto\", cmap=cmap, alpha=0.3, vmin=0, vmax=len(palette))\n",
    "    axs[0].set_yticks([]); axs[1].set_yticks([])\n",
    "    axs[0].set_title(f\"{date}; Beh\"); axs[1].set_title(\"Inferred by rSLDS\")\n",
    "    axs[1].set_xticks([])\n",
    "    \n",
    "\n",
    "    ###plot RID; RIB; AVB \n",
    "    neurons = [\"RID\", \"RIB\", \"AVB\"]\n",
    "    colors = [\"darkred\", \"crimson\", \"purple\"]\n",
    "    neuron_to_color = dict(zip(neurons, colors))\n",
    "    plot_states_and_neurons(neurons, neuron_to_color, z_w, q_z_w, fig = fig, axs = axs[2:4])\n",
    "    \n",
    "    \n",
    "    neurons = [\"RIM\", \"AIB\", \"AVA\"]\n",
    "    colors = [\"dodgerblue\", \"darkmagenta\", \"navy\"]\n",
    "    neuron_to_color = dict(zip(neurons, colors))\n",
    "    plot_states_and_neurons(neurons, neuron_to_color, z_w, q_z_w, fig = fig, axs = axs[4:6])\n",
    "    \n",
    "        \n",
    "    ###plot AIY; RIM; RIB \n",
    "    neurons = [\"OLQ\", \"URY\", \"OLL\", \"RIV\"]\n",
    "    colors = [\"olivedrab\", \"seagreen\", \"darkslategray\", \"purple\"]\n",
    "    neuron_to_color = dict(zip(neurons, colors))\n",
    "    plot_states_and_neurons(neurons, neuron_to_color, z_w, q_z_w, fig = fig, axs = axs[6:])\n",
    "    \n",
    "\n",
    "    all_figs.append(fig)\n",
    "\n",
    "# with PdfPages(os.path.join(pdf_save_dir,f\"all_dates.pdf\")) as pdf:\n",
    "#     for fig in all_figs: \n",
    "#         pdf.savefig(fig)\n",
    "#         # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f176aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cec86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import itertools\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "    \n",
    "# 1. Extract feature means for each occurrence\n",
    "from collections import defaultdict\n",
    "occurrence_means = defaultdict(list)\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "\n",
    "\n",
    "def get_pairwise_distances(f1_means, f2_means, n_permutations = 1000 ):\n",
    "    # Pairwise comparisons\n",
    "    # rand_i =\n",
    "    rand_i = np.array([random.randint(0,  f2_means.shape[0]-1) for _ in range(n_permutations)])\n",
    "    within_dists = []\n",
    "    between_dists = []\n",
    "    # comb_is = [[[v1,v2] for v1 in range(len(f1_means))] for v2 in range(len(f2_means))]\n",
    "    comb_is = np.array(list(itertools.product( range(len(f1_means)),  range(len(f2_means)))))[rand_i]\n",
    "    for i, j in comb_is:\n",
    "        dist = f1_means[i] - f2_means[i]\n",
    "        # # dist = np.abs(X_occ[i] - X_occ[j])  # or use squared distance\n",
    "        # if labels_occ[i] == labels_occ[j]:\n",
    "        #     within_dists.append(dist)\n",
    "        # else:\n",
    "        between_dists.append(dist)\n",
    "\n",
    "    # Convert to arrays\n",
    "    # within_dists = np.array(within_dists)/\n",
    "    between_dists = np.array(between_dists)\n",
    "    return between_dists\n",
    "\n",
    "\n",
    "def get_means_from_occurances(match_start_idx, match_end_idx, features):\n",
    "    occurrence_means = []\n",
    "    for start, end in zip(match_start_idx, match_end_idx):\n",
    "       \n",
    "        mean_feat = features[start:end].nanmean(axis=0)  # mean over this occurrence\n",
    "        occurrence_means.append(mean_feat)\n",
    "    occurrence_means = np.vstack(occurrence_means)\n",
    "    return occurrence_means\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_runs(x):\n",
    "    \"\"\"Find start indices, end indices, and values of runs in a 1D array.\"\"\"\n",
    "    n = len(x)\n",
    "    if n == 0:\n",
    "        return np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=x.dtype)\n",
    "\n",
    "    change_idx = np.diff(x, prepend=x[0]-1).nonzero()[0]\n",
    "    start_idx = change_idx\n",
    "    end_idx = np.append(change_idx[1:], n)\n",
    "    values = x[start_idx]\n",
    "    return start_idx, end_idx, values\n",
    "def filter_runs_for_duration(start_idx, end_idx, thresh):\n",
    "    good_is = np.argwhere(end_idx - start_idx > thresh).flatten()\n",
    "    return start_idx[good_is], end_idx[good_is]\n",
    "\n",
    "\n",
    "def plot_feature_avg_diff_heatmap(f1_means, f2_means, feature_labels=None, title=\"Feature Differences\"):\n",
    "    # 1. Compute mean difference\n",
    "    mean_diff = np.mean(f1_means, axis=0) - np.mean(f2_means, axis=0)\n",
    "    abs_diff = np.abs(mean_diff)\n",
    "\n",
    "    # 2. Sort by magnitude\n",
    "    sort_idx = np.argsort(abs_diff)[::-1]\n",
    "    sorted_diff = mean_diff[sort_idx]\n",
    "\n",
    "    if feature_labels is None:\n",
    "        feature_labels = [f\"feat_{i}\" for i in range(len(mean_diff))]\n",
    "    sorted_labels = np.array(feature_labels)[sort_idx]\n",
    "\n",
    "    # 3. Plot as heatmap\n",
    "    plt.figure(figsize=(10, 1 + 0.2 * len(sorted_diff)))\n",
    "    sns.heatmap(sorted_diff[:, np.newaxis], annot=True, fmt=\".2f\", cmap=\"vlag\",\n",
    "                yticklabels=sorted_labels, xticklabels=[\"Mean Diff (match - miss)\"], cbar=True)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_diff_summary(f1_means, f2_means, feature_labels=None, title=\"Feature Differences\"):\n",
    "    # 1. Compute mean difference\n",
    "    mean_diff = np.mean(f1_means, axis=0) - np.mean(f2_means, axis=0)\n",
    "    abs_diff = np.abs(mean_diff)\n",
    "\n",
    "    # 2. Sort by absolute magnitude\n",
    "    sort_idx = np.argsort(abs_diff)[::-1]\n",
    "    sorted_diff = mean_diff[sort_idx]\n",
    "\n",
    "    # 3. Handle labels\n",
    "    if feature_labels is None:\n",
    "        feature_labels = [f\"feat_{i}\" for i in range(len(mean_diff))]\n",
    "    sorted_labels = np.array(feature_labels)[sort_idx]\n",
    "\n",
    "    # 4. Plot: heatmap and bar graph\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 0.4 * len(sorted_diff) + 2), gridspec_kw={'width_ratios': [1, 2]})\n",
    "\n",
    "    # 4a. Heatmap\n",
    "    sns.heatmap(sorted_diff[:, np.newaxis], annot=True, fmt=\".2f\", cmap=\"vlag\",\n",
    "                yticklabels=sorted_labels, xticklabels=[\"Diff\"], cbar=True, ax=axs[0])\n",
    "    axs[0].set_title(\"Heatmap\")\n",
    "\n",
    "    # 4b. Bar graph\n",
    "    axs[1].barh(np.arange(len(sorted_diff)), sorted_diff, color='skyblue', edgecolor='k')\n",
    "    axs[1].set_yticks(np.arange(len(sorted_diff)))\n",
    "    axs[1].set_yticklabels(sorted_labels)\n",
    "    axs[1].invert_yaxis()  # Highest diff on top\n",
    "    axs[1].set_xlabel(\"Mean Difference (F1 - F2)\")\n",
    "    axs[1].set_title(\"Bar Plot\")\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_pairwise_distance_heatmaps(within_dists, between_dists, feature_labels=None, title=\"Pairwise Distance Summary\"):    \n",
    "# def plot_pairwise_distance_heatmaps(f1_means, f2_means, feature_labels=None, title=\"Pairwise Distance Summary\"):\n",
    "    # 1. Compute mean pairwise distances per feature\n",
    "    # within_dists = get_pairwise_distances(f1_means, f1_means)   # shape (n_features,)\n",
    "    # between_dists = get_pairwise_distances(f1_means, f2_means)  # shape (n_features,)\n",
    "    diff_dists = between_dists - within_dists                   # shape (n_features,)\n",
    "\n",
    "    # 2. Sort by difference\n",
    "    sort_idx = np.argsort(diff_dists)[::-1]\n",
    "\n",
    "    within_sorted = within_dists[sort_idx]\n",
    "    between_sorted = between_dists[sort_idx]\n",
    "    diff_sorted = diff_dists[sort_idx]\n",
    "\n",
    "    if feature_labels is None:\n",
    "        feature_labels = [f\"feat_{i}\" for i in range(len(diff_dists))]\n",
    "    labels_sorted = np.array(feature_labels)[sort_idx]\n",
    "\n",
    "    # 3. Stack for heatmap display\n",
    "    data_matrix = np.vstack([\n",
    "        within_sorted,\n",
    "        between_sorted,\n",
    "        diff_sorted\n",
    "    ]).T  # shape (n_features, 3)\n",
    "\n",
    "    # 4. Plot heatmap\n",
    "    plt.figure(figsize=(8, 0.5 * len(labels_sorted) + 2))\n",
    "    sns.heatmap(data_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\",\n",
    "                xticklabels=[\"Within\", \"Between\", \"Diff (B - W)\"],\n",
    "                yticklabels=labels_sorted, cbar=True)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "##1) concatenate beh and rsdls states \n",
    "duration_thresh = 1 \n",
    "beh_states = [0,1,2]\n",
    "for beh_state in [0,1,2]:\n",
    "    for rsdls_state in np.difference(beh_states, beh_state):\n",
    "        true_pos = np.logical_and(z == beh_state, q_z == beh_state)\n",
    "        match_start_idx, match_end_idx, _ = find_runs(true_pos)\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.hist(match_end_idx-match_start_idx, bins = 50)\n",
    "        ax.set_title(f\"{beh_state} match run duraitons \")\n",
    "        \n",
    "        miss =  np.logical_and(z == beh_state, q_z == rsdls_state)\n",
    "        miss_start, miss_end , _ = find_runs(true_pos)\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.hist(miss_end-miss_start, bins = 50)\n",
    "        ax.set_title(f\"{beh_state} miss {rsdls_state} run duraitons \")\n",
    "        \n",
    "        \n",
    "        #plot run lengths \n",
    "        \n",
    "        \n",
    "        \n",
    "        # get runs longer thanduraiton thrsh + pltot o make sure ok\n",
    "        match_start_idx, match_end_idx = filter_runs_for_duration(match_start_idx, match_end_idx, duration_thresh)\n",
    "        match_neural_means = get_means_from_occurances(match_start_idx, match_end_idx, neural_features)\n",
    "        match_beh_means = get_means_from_occurances(match_start_idx, match_end_idx, beh_features)\n",
    "                                    \n",
    "                                    \n",
    "        miss_start, miss_end  = filter_runs_for_duration(miss_start, miss_end , duration_thresh)\n",
    "        miss_neural_means = get_means_from_occurances(match_start_idx, match_end_idx, neural_features)\n",
    "        miss_beh_means = get_means_from_occurances(match_start_idx, match_end_idx, beh_features)\n",
    "        \n",
    "        ax.hist(match_end_idx-match_start_idx, bins = 50)\n",
    "        ax.set_title(f\"{beh_state} match run duraitons filtered\")\n",
    "        ax.hist(miss_end-miss_start, bins = 50)\n",
    "        ax.set_title(f\"{beh_state} miss {rsdls_state} run duraitons filtered\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        f1_means = match_neural_means\n",
    "        f2_means = miss_neural_means\n",
    "        \n",
    "        # option1 Compute difference in means across occurrences;#plot heatmap of distances \n",
    "        plot_feature_avg_diff_heatmap(f1_means, f2_means, feature_labels=neural_labels, title=\"Feature Differences\")\n",
    "        plot_feature_diff_summary(f1_means, f2_means, feature_labels=neural_labels, title=\"Feature Differences\")\n",
    "\n",
    "        #cotpion 2 vs pairwise differneces? \n",
    "        within_dists = get_pairwise_distances(f1_means, f1_means,  n_permutations = 1000)\n",
    "        between_dists = get_pairwise_distances(f1_means, f2_means,  n_permutations = 1000)\n",
    "        plot_pairwise_distance_heatmaps(within_dists, between_dists, feature_labels=neural_labels, title=\"Pairwise Distance Summary\")\n",
    "        \n",
    "        \n",
    "        f1_means = match_beh_means\n",
    "        f2_means = miss_beh_means\n",
    "        beh_labels = [\"accel\", \"vel\", \"curvature\"]\n",
    "        \n",
    "                # option1 Compute difference in means across occurrences;#plot heatmap of distances \n",
    "        plot_feature_avg_diff_heatmap(f1_means, f2_means, feature_labels=neural_labels, title=\"Feature Differences\")\n",
    "        plot_feature_diff_summary(f1_means, f2_means, feature_labels=neural_labels, title=\"Feature Differences\")\n",
    "\n",
    "        #cotpion 2 vs pairwise differneces? \n",
    "        within_dists = get_pairwise_distances(f1_means, f1_means,  n_permutations = 1000)\n",
    "        between_dists = get_pairwise_distances(f1_means, f2_means,  n_permutations = 1000)\n",
    "        plot_pairwise_distance_heatmaps(within_dists, between_dists, feature_labels=neural_labels, title=\"Pairwise Distance Summary\")\n",
    "        \n",
    "        ##plot heatmap of distances  ; sorting by ratio of distance  \n",
    "\n",
    "        ##plot heatmap of distances  ; sorting by diffence of distance  \n",
    "\n",
    "\n",
    "        # # 5. Sort features by importance\n",
    "        # top_k = 10\n",
    "        # top_idx = np.argsort(np.abs(mean_diff))[::-1][:top_k]\n",
    "\n",
    "        # for i in top_idx:\n",
    "        #     print(f\"Feature {i}: Mean A={X_A[:, i].mean():.3f}, Mean B={X_B[:, i].mean():.3f}, \"\n",
    "        #         f\"Diff={mean_diff[i]:.3f}, p={p_vals[i]:.3e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "       \n",
    "       ###for each occurance get mean of features (neuron acitvities); max of features; Mean of first x frames; and compare to control \n",
    "       \n",
    "       \n",
    "       \n",
    "       \n",
    "##3) \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c983f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color ethograms by false neg afla pos etc \n",
    "# color ethograms by false neg afla pos etc \n",
    "\n",
    "\n",
    "\n",
    "##remove neuron ? s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86783592",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces, neural_labels, behavior_classification, mask = load_all_data_but_pretend_its_all_one_worm()\n",
    "\n",
    "velocity = np.array([full_beh_data[i][\"velocity\"][0:1599] for i in range(len(full_beh_data))])\n",
    "acceleration = np.array([full_beh_data[i][\"acceleration\"][1:1600] for i in range(len(full_beh_data))])\n",
    "head_curvature = np.array([full_beh_data[i][\"head_angle\"][0:1599] for i in range(len(full_beh_data))])\n",
    "worm_curvature = np.array([full_beh_data[i][\"worm_curvature\"][0:1599] for i in range(len(full_beh_data))])\n",
    "pumping = np.array([full_beh_data[i][\"pumping\"][0:1599] for i in range(len(full_beh_data))])\n",
    "\n",
    "q_z_all = np.concatenate(q_z)\n",
    "z_all = np.concatenate(z)\n",
    "\n",
    "print(traces.shape)\n",
    "print(neural_labels.shape)\n",
    "print(acceleration.shape)\n",
    "print(q_z_all.shape)\n",
    "\n",
    "for i in range(len(exp_dates)):\n",
    "    \n",
    "    n_sections = 42\n",
    "    # fig, axs = plt.subplots(2,1, figsize=(18,6))\n",
    "    fig, axs = plt.subplots(2*n_sections,1, figsize=(18,6*n_sections))\n",
    "    axs[0].imshow(z_w[None,:], aspect=\"auto\", cmap=cmap, alpha=0.3, vmin=0, vmax=len(palette))\n",
    "    axs[1].imshow(q_z_w[None,:], aspect=\"auto\", cmap=cmap, alpha=0.3, vmin=0, vmax=len(palette))\n",
    "    axs[0].set_yticks([]); axs[1].set_yticks([])\n",
    "    axs[0].set_title(f\"{date}; Beh\"); axs[1].set_title(\"Inferred by rSLDS\")\n",
    "    axs[1].set_xticks([])\n",
    "    \n",
    "    z_w = z_all[1599*i:(i+1)*1599]\n",
    "    q_z_w = z_all[1599*i:(i+1)*1599]\n",
    "    traces_w = traces[1599*i:(i+1)*1599]\n",
    "\n",
    "    ###plot RID; RIB; AVB \n",
    "    neurons = [\"RID\", \"RIB\", \"AVB\"]\n",
    "    colors = [\"darkred\", \"crimson\", \"purple\"]\n",
    "    neuron_to_color = dict(zip(neurons, colors))\n",
    "    plot_states_and_neurons(neurons, neuron_to_color, z_w, q_z_w, traces_w,labels,  fig = fig, axs = axs[2:4])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870e967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    #set up behavior classification dict\n",
    "    behavior_classification = dict()\n",
    "    behavior_classification[\"is_turn\"] = np.zeros(T*len(full_traces))\n",
    "    behavior_classification[\"is_pause\"] = np.zeros(T*len(full_traces))\n",
    "    behavior_classification[\"is_rev\"] = np.zeros(T*len(full_traces))\n",
    "    behavior_classification[\"is_fwd\"] = np.zeros(T*len(full_traces))\n",
    "    behavior_classification[\"is_revturn\"] = np.zeros(T*len(full_traces))\n",
    "    behavior_classification[\"is_purerev\"] = np.zeros(T*len(full_traces))\n",
    "    behavior_classification[\"is_pureturn\"] = np.zeros(T*len(full_traces))\n",
    "    behavior_classification[\"is_rev_of_rev_turn\"] = np.zeros(T*len(full_traces))\n",
    "    behavior_classification[\"is_turn_of_rev_turn\"] = np.zeros(T*len(full_traces))\n",
    "\n",
    "    #fill it in\n",
    "    w=0\n",
    "    for bc in full_beh_classification:\n",
    "        for key in bc.keys():\n",
    "            behavior_classification[key][w*T:(w+1)*T] = bc[key][1:(T+1)] # shifting by one bc of the trace\n",
    "        w+=1 #update worm index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3caabe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_all_data_but_pretend_its_all_one_worm():\n",
    "    # This function concatenates everything into one worm and also returns a mask that tells you which neurons are and arent present\n",
    "    # Thing to ponder: adding columns of NaNs between worms would probably help the rSLDS learn better\n",
    "    full_traces, full_neural_labels, full_beh_classification, full_beh_data = load_all_data()\n",
    "\n",
    "    #get list of all neurons recorded in at least one trial\n",
    "    neural_labels_set = set()\n",
    "    for nl in full_neural_labels:\n",
    "        neural_labels_set = neural_labels_set.union(set(nl))\n",
    "    neural_labels = np.sort(list(neural_labels_set))\n",
    "\n",
    "    #now make the traces array, where the different recordings are concatenated and matched by neuron\n",
    "    #note: a couple of worms have 15 extra timesteps. I am truncating those to make things easier down the line\n",
    "    T = 1599\n",
    "    traces = np.zeros((T*len(full_traces), neural_labels.shape[0]))*np.nan #initialize traces to nan \n",
    "    #build the traces matrix\n",
    "    w=0\n",
    "    for tr, labels, in zip(full_traces, full_neural_labels): #for each worm, w\n",
    "        for i in range(tr.shape[1]): #for each neuron, i\n",
    "            label = labels[i] #get neuron name\n",
    "            idx = np.where(neural_labels==label)[0][0] #get index in full array\n",
    "            traces[w*T:(w+1)*T, idx] = tr[0:T,i] #put the neuron's activity in the appropriate spot\n",
    "        w+=1 #update worm index\n",
    "\n",
    "    \n",
    "\n",
    "    #set up behavior classification dict\n",
    "    behavior_classification = dict()\n",
    "    behavior_classification[\"is_turn\"] = np.zeros(T*len(full_traces))\n",
    "    behavior_classification[\"is_pause\"] = np.zeros(T*len(full_traces))\n",
    "    behavior_classification[\"is_rev\"] = np.zeros(T*len(full_traces))\n",
    "    behavior_classification[\"is_fwd\"] = np.zeros(T*len(full_traces))\n",
    "    behavior_classification[\"is_revturn\"] = np.zeros(T*len(full_traces))\n",
    "    behavior_classification[\"is_purerev\"] = np.zeros(T*len(full_traces))\n",
    "    behavior_classification[\"is_pureturn\"] = np.zeros(T*len(full_traces))\n",
    "    behavior_classification[\"is_rev_of_rev_turn\"] = np.zeros(T*len(full_traces))\n",
    "    behavior_classification[\"is_turn_of_rev_turn\"] = np.zeros(T*len(full_traces))\n",
    "\n",
    "    #fill it in\n",
    "    w=0\n",
    "    for bc in full_beh_classification:\n",
    "        for key in bc.keys():\n",
    "            behavior_classification[key][w*T:(w+1)*T] = bc[key][1:(T+1)] # shifting by one bc of the trace\n",
    "        w+=1 #update worm index\n",
    "\n",
    "    #return mask of nan data\n",
    "    mask =  (~np.isnan(traces)).astype(int)\n",
    "    return traces, neural_labels, behavior_classification, mask"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
